{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c8hK3tzdo8k_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a0a5fb-7c78-4a56-c2a3-baf0f5e3df0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_id = \"10owNxo07iXD2Al0IFtifrNBAjeoyWpHSOV18ycatZ8U\"\n",
        "worksheet_name = \"Sheet1\""
      ],
      "metadata": {
        "id": "gqbGMon3GDaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code defines a Google Sheets ID (sheet_id) and a worksheet name (worksheet_name) for future reference"
      ],
      "metadata": {
        "id": "uBVL_7kAqNJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gspread\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "R-b6rv4jGExp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import andrews_curves\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import os\n",
        "from google.colab import files\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.impute import SimpleImputer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "\n",
        "path_train = '/content/drive/MyDrive/Data/train_data.csv'\n",
        "train_df = pd.read_csv(path_train)\n",
        "\n",
        "path_val = '/content/drive/MyDrive/Data/val_data.csv'\n",
        "val_df = pd.read_csv(path_val)"
      ],
      "metadata": {
        "id": "eF3NqkOzslq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries to set up the notebook, and getting the dataset ready to be used by the AI model."
      ],
      "metadata": {
        "id": "I2Xb7e-0rCMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "sh = gc.open_by_key(sheet_id)\n",
        "sh.worksheets()\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "worksheet.get_all_records()\n",
        "df = pd.DataFrame(worksheet.get_all_records())"
      ],
      "metadata": {
        "id": "-vM1pUGQGKb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code retrieves all records from the worksheet, and converts them into a pandas DataFrame (df)."
      ],
      "metadata": {
        "id": "5xucqHwgrMnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.columns.difference(['diabetes'])\n",
        "y = ['diabetes']\n",
        "\n",
        "X_train = train_df[X]\n",
        "print('X_train, our input variables:')\n",
        "print(X_train.head())\n",
        "print()\n",
        "\n",
        "y_train = train_df[y]\n",
        "print('y_train, our output variable:')\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6qLKkduso5p",
        "outputId": "d4b140f4-bab4-4387-eef2-d856bd05fc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train, our input variables:\n",
            "   HbA1c_level   age  blood_glucose_level    bmi  gender  heart_disease  \\\n",
            "0          4.0  36.0                  145  17.06     1.0              0   \n",
            "1          6.1  58.0                  159  37.83     0.0              0   \n",
            "2          5.0  43.0                  160  39.20     0.0              0   \n",
            "3          5.7  67.0                  159  28.39     1.0              0   \n",
            "4          6.0   5.0                   80  27.32     0.0              0   \n",
            "\n",
            "   hypertension  smoking_history_current  smoking_history_ever  \\\n",
            "0             0                        0                     0   \n",
            "1             0                        0                     0   \n",
            "2             0                        0                     0   \n",
            "3             1                        0                     0   \n",
            "4             0                        0                     0   \n",
            "\n",
            "   smoking_history_former  smoking_history_never  smoking_history_not current  \n",
            "0                       0                      1                            0  \n",
            "1                       0                      0                            0  \n",
            "2                       0                      1                            0  \n",
            "3                       0                      1                            0  \n",
            "4                       0                      0                            0  \n",
            "\n",
            "y_train, our output variable:\n",
            "   diabetes\n",
            "0         0\n",
            "1         0\n",
            "2         0\n",
            "3         0\n",
            "4         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the model by mentioning which columns represent the input features and which column is the output label."
      ],
      "metadata": {
        "id": "4dR3ULs2rsKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "pnsv974Fj4sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adasyn = ADASYN(random_state=42)\n",
        "X_adasyn, y_adasyn = adasyn.fit_resample(X_train_imputed, y_train)"
      ],
      "metadata": {
        "id": "E-Ev3AMWs64j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = val_df[X]\n",
        "y_val = val_df[y]"
      ],
      "metadata": {
        "id": "X8bVtVjts9ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_adasyn_scaled = scaler.fit_transform(X_adasyn)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "VTrEuC6JtHD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses a StandardScaler to standardize the features of two datasets, X_adasyn and X_val. The fit_transform method scales the training data (X_adasyn), while the transform method scales the validation data (X_val) using the parameters learned from the training data."
      ],
      "metadata": {
        "id": "JzVloCBjsTC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pandas DataFrames to PyTorch tensors\n",
        "X_adasyn_tensor = torch.tensor(X_adasyn_scaled, dtype=torch.float32)\n",
        "y_adasyn_tensor = torch.tensor(y_adasyn.values, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "zifR0aGeru3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code converts the standardized datasets (X_adasyn_scaled, y_adasyn, X_val_scaled, y_val) into PyTorch tensors (X_adasyn_tensor, y_adasyn_tensor, X_val_tensor, y_val_tensor) with a specified data type of float32."
      ],
      "metadata": {
        "id": "dyJyhPQTsjOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiabetesPredictionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DiabetesPredictionModel, self).__init__()\n",
        "        self.conv_stack = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the output size of the convolutional layers\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, input_size)\n",
        "            conv_output_size = self._get_conv_output_size(dummy_input)\n",
        "\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(conv_output_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "    def _get_conv_output_size(self, x):\n",
        "        x = self.conv_stack(x)\n",
        "        return x.view(x.size(0), -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_stack(x)\n",
        "        logits = self.linear_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "R35FrMiarxLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a neural network model (DiabetesPredictionModel) using PyTorch's nn.Module. It consists of convolutional layers followed by batch normalization, ReLU activation, and max pooling. The convolutional layers are then flattened, and the output is passed through fully connected layers with ReLU activation, dropout, and a final linear layer for binary classification. The model is designed for predicting diabetes based on input data with a specified size."
      ],
      "metadata": {
        "id": "2jva1swTt801"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset for training and testing\n",
        "class DiabetesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "metadata": {
        "id": "7zqpJz9AtjMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a PyTorch dataset class (DiabetesDataset) for managing input features (X) and labels (y). It includes methods for accessing individual items and obtaining the dataset's length."
      ],
      "metadata": {
        "id": "W1GZoizPuWgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/drive/MyDrive/NN_Models', exist_ok=True)"
      ],
      "metadata": {
        "id": "-gGt3bdBPREG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training and validation\n",
        "adasyn_dataset = DiabetesDataset(X_adasyn_tensor.unsqueeze(1), y_adasyn_tensor)  # Add an extra dimension for input channels\n",
        "val_dataset = DiabetesDataset(X_val_tensor.unsqueeze(1), y_val_tensor)  # Add an extra dimension for input channels\n",
        "adasyn_loader = DataLoader(adasyn_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "PmuGxEQ_TAC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up PyTorch datasets (adasyn_dataset and val_dataset) and corresponding data loaders (adasyn_loader and val_loader). The input features are adjusted to include an additional dimension for input channels using unsqueeze(1). The DataLoader is configured with batch sizes and optional shuffling for the training dataset."
      ],
      "metadata": {
        "id": "N4oWyYVSu5ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_list_length = 5\n",
        "\n",
        "# Define the run_model_experiment function\n",
        "def run_model_experiment(experiment_number):\n",
        "    # Generate random learning rate, factor, and patience values\n",
        "    lr = random.uniform(1e-5, 1e-3)\n",
        "    hyp1 = random.uniform(0.05, 0.9)\n",
        "    hyp2 = random.randint(5, 15)\n",
        "\n",
        "    # Instantiate the model\n",
        "    input_size = X_adasyn_tensor.shape[1]\n",
        "    model = DiabetesPredictionModel(input_size)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=hyp1, patience=hyp2, verbose=True)\n",
        "\n",
        "    num_epochs = 5\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in adasyn_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "        train_loss /= len(adasyn_dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "        val_loss /= len(val_dataset)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/Models/model_{experiment_number}.pt')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(f'/content/drive/MyDrive/Models/model_{experiment_number}.pt'))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_adasyn = (model(X_adasyn_tensor.unsqueeze(1)) > 0.5).squeeze().numpy()\n",
        "        y_pred_val = (model(X_val_tensor.unsqueeze(1)) > 0.5).squeeze().numpy()\n",
        "\n",
        "    val_accuracy = round(accuracy_score(y_val, y_pred_val) * 100, 1)\n",
        "\n",
        "    # Calculate diabetes prediction accuracy\n",
        "    cm_val = confusion_matrix(y_val, y_pred_val)\n",
        "\n",
        "    diabetes_accuracy_val = round(cm_val[1][1] / (cm_val[1][1] + cm_val[1][0]) * 100, 1)\n",
        "\n",
        "    print(\"Val Accuracy:\", val_accuracy)\n",
        "\n",
        "    print(\"Val Diabetes Accuracy:\", diabetes_accuracy_val)\n",
        "\n",
        "    # Format lr and accuracies for Google Sheets\n",
        "    lr_str = \"{:.2E}\".format(lr)\n",
        "    val_accuracy_str = str(val_accuracy) + '%'\n",
        "    diabetes_accuracy_val_str = str(diabetes_accuracy_val) + '%'\n",
        "\n",
        "    # Add experiment details to Google Sheets\n",
        "    worksheet.append_row([experiment_number, lr_str, hyp1, hyp2, val_accuracy_str, diabetes_accuracy_val_str])\n",
        "\n",
        "adasyn_loader = DataLoader(adasyn_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "# Run experiments\n",
        "experiment_number = 42 # change after every run\n",
        "for _ in range(max_list_length):\n",
        "    run_model_experiment(experiment_number)\n",
        "    experiment_number += 1\n"
      ],
      "metadata": {
        "id": "Vv1FBU1vRNBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function (run_model_experiment) that runs a machine learning experiment with a randomized set of hyperparameters. It trains a diabetes prediction model using PyTorch, monitors validation loss, and saves the best model. The experiments are conducted multiple times, and the results, including accuracy metrics, are printed and stored in the Google Sheet named as 'CNN Model tracking'."
      ],
      "metadata": {
        "id": "AtZ_EUDmvUzf"
      }
    }
  ]
}